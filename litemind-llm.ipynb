{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and reading in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open (\"the-verdict.txt\",'r',encoding=\"utf-8\") as f :\n",
    "    raw_text=f.read()\n",
    "print(\"Total number of characters:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed= re.split(r'(,.:;?!\"()\\']|--|\\s)',text)\n",
    "        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed=[\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<|unk|>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer= SimpleTokenizerV2(vocab)\n\u001b[32m      2\u001b[39m text=\u001b[33m\"\u001b[39m\u001b[33mHello It\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the last time I am going to see you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ids=\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimpleTokenizerV2.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      8\u001b[39m preprocessed=[item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m      9\u001b[39m preprocessed=[\n\u001b[32m     10\u001b[39m     item \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.str_to_int \n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m<|unk|>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[32m     12\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids=[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: '<|unk|>'"
     ]
    }
   ],
   "source": [
    "tokenizer= SimpleTokenizerV2(vocab)\n",
    "text=\"Hello It's the last time I am going to see you\"\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer.decode(\u001b[43mids\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ids' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bytepair Encoding simillar to Gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.12/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 23748, 703, 389, 345, 30, 220, 50256, 383, 995, 318, 257, 4950, 1295]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "text=\"Hi hello how are you? <|endoftext|> The world is a beautiful place\"\n",
    "integers=tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi hello how are you? <|endoftext|> The world is a beautiful place\n"
     ]
    }
   ],
   "source": [
    "string=tokenizer.decode(integers)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "#length of the input\n",
    "# model looks at the 4 words and then predict the next word\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for  i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "    print(context,\"---->\",desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for  i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "    print(tokenizer.decode(context),\"---->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[] #input chunk \n",
    "        self.target_ids=[] #output chunk \n",
    "\n",
    "        token_ids=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids)>max_length,\"Number of token_ids must be atleast equal to max_length + 1\"\n",
    "        #sliding window \n",
    "        for i in range(0,len(token_ids)-max_length,stride): \n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            output_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #mapping dataset map each input to corresponding output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,  #batch_size=4 no of elements in a training batch , stride=128 means the amount of movement of the window\n",
    "            max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0): # drop last is done so that the last batch of the training set may be ignored as if the batch size< 4 causes problem num_workers is the number of cpu threads that can be run simultaneously\n",
    "            tokenizer=tiktoken.get_encoding(\"gpt2\") #tokenizer initiated\n",
    "            dataset=GPTDatasetV1(txt,tokenizer,max_length,stride) #dataset made\n",
    "            dataloader= DataLoader( #create dataloader\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                drop_last=drop_last,   #this dataloaderV1 and then from getitems it gives out a input output\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "            return dataloader\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf_8\") as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "dataloader=create_dataloader_v1(\n",
    "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
    ")\n",
    "data_iter=iter(dataloader)\n",
    "first_batch=next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch=next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=4,stride=4,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"Targets:\\n\",targets)\n",
    "# each of the tensor contains 4 tokens since the max_length is also 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1]) #consider they are token ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123) #generate random in any device\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) #embedding layer of 6*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight) #embedding layer is an efficient way to implement one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) #convert token id 3 into a vector of 3 dimension and print it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding=token_embedding_layer(inputs)\n",
    "# print(token_embedding) # 50257 *256 50257 tokens and 256 dimensions\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length #another embedding layer because gpt 2 uses position embeddings\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embedding=pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embedding.shape)\n",
    "print(pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#input embedding will be posn embedding + token embedding \n",
    "input_embedding=pos_embedding+token_embedding\n",
    "print(input_embedding.shape)\n",
    "# print(input_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have completed preprocessing, tokenization , and token embeddings with positional embeddings\n",
    "#simple self attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "#normalising\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#softmax normalization\n",
    "# e^x /(e^x1+....e^xt)\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)# summation of eac row   \n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#pytorch softmax because naive softmax can have underflow or overflow\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)# final context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#this is for just 'journey' now lets do this for each text\n",
    "attn_scores = torch.empty(len(inputs), len(inputs))\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#the above operation works but is very complex and not scalable\n",
    "#but according to linear algebra\n",
    "attn_score = inputs @ inputs.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs= attn_weights @ inputs\n",
    "print(all_context_vecs) #literally the same scaling operation as before(write down and check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we can do this then why do we need trainable weights? \n",
    "because there is meaning in the current sentence, like here there is almost no relation between one and journey but consider there might be in this specific context, so to represent that we need to have trainable weights\n",
    "\n",
    "##### eg: the cat sat on the mat because it was warm -> if warm is the query then mat needs to have a higher attention score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing self-attention with trainable weights\n",
    "\n",
    "\n",
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "\n",
    "#in gpt like models the input and output dimensions are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "#initialize weight matrices \n",
    "torch.manual_seed(123)\n",
    "# we need to learn how to convert input vectors into key query and value vectors\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#when using weight matrices for model training \n",
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_2 = x_2 @ W_query # x_2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "query.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "queries= inputs @ W_query\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(\"query.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "#calcultating attention score\n",
    "\n",
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2= query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores =  queries @ keys.T #all queries with attention \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# now normalisation \n",
    "# attention weights- they should sum up to 1\n",
    "# first scale by root d-keys and then softmax\n",
    "#why dividibng with root of d_k? it gives a stable output\n",
    "#relared to variance\n",
    "d_k= keys.shape[-1]\n",
    "attn_weights_2=torch.softmax(attn_scores_2/d_k**0.5,dim =-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a self attention python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_V1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value=nn.Parameter(torch.rand(d_in,d_out))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        keys=x @ self.W_key\n",
    "        queries=x @ self.W_query\n",
    "        values=x @ self.W_value\n",
    "        attn_score=queries @ keys.T\n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vec= attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1=SelfAttention_V1(d_in,d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) #stable\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out) #weight initialization is more sophisticates\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# mask out future tokens\n",
    "# we mask out the attention weights above the diagonal and then we normalise the attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores= queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim =-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# now we generate a mask \n",
    "# so we basically have the output(attention weights) and then mask it then normalize rows\n",
    "context_length= attn_scores.shape[0]\n",
    "mask_simple=torch.tril(torch.ones(context_length,context_length))# tril creates a lower triangular matrix\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple= attn_weights * mask_simple #canceling influence of future tokens (data leakage) this is bad\n",
    "print(masked_simple) #now normalise it to have each row sum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums=masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums \n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) #fill the positive values with -inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#code should be able to handle batches\n",
    "# we will simulate batch inputs for now we are using 2 inputs with 6 tokens and each token has embedding dimension of 3\n",
    "batch = torch.stack((inputs, inputs),dim=0)\n",
    "print(batch.shape)\n",
    "# the causal attention class will be simillar to self attention but we now have dropout and causal masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "ca = CausalAttention(d_in,d_out,context_length,0.0)\n",
    "context_vecs=ca(batch)\n",
    "print(\"Context_vecs.shape:\",context_vecs.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs),dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1(one type of mutihead attention)\n",
    "# divide attention mechanism into \"multiple heads\" each operating independently \n",
    "# instead of a single key,query and value matrix we have multiple (for now lets just say 2)\n",
    "# hence we get 2 attention weights and then 2 context vectors that will be then combined to a single vector\n",
    "\n",
    "# consider each context vec to have 6 x 2 dimension so the combined context vector will have the dimension 6 x 4\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "# this current system works well but the single heads are processed sequentially and then aggregated, we can improve them by making this in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multihead attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # we have to group by the number of heads\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Context_vecs Shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "inputs=torch.tensor(\n",
    "    [[0.43,0.15,0.89,0.55,0.87,0.66],\n",
    "     [0.57,0.85,0.64,0.22,0.58,0.33],\n",
    "     [0.77,0.25,0.10,0.05,0.80,0.55]]\n",
    ")\n",
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch.shape)\n",
    "batch_size,context_length,d_in=batch.shape\n",
    "d_out=6\n",
    "mha=MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads=2)\n",
    "context_vecs=mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context_vecs Shape:\",context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to make a placeholder for gpt model\n",
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257, #vocabulary\n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768, #embedding dimension\n",
    "    \"n_heads\": 12, #no of attention heads\n",
    "    \"n_layers\": 12, #no of layers\n",
    "    \"drop_rate\": 0.1, #dropout rate \n",
    "    \"qkv_bias\": False #Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy GPT Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        #placeholder transformer\n",
    "        self.trf_blocks=nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        #placeholder layernorm \n",
    "        self.final_norm =DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_indices = torch.arange(seq_len, device=in_idx.device)\n",
    "        pos_embeds = self.pos_emb(pos_indices)\n",
    "        # pos_embeds=self.pos_emb(in_idx)\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        #placeholder\n",
    "    def forward(self,x):\n",
    "        #nothing happens here just returning the input\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def  __init__(self,normalized_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "#tokenization \n",
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1=\"Every effort moves you\"\n",
    "txt2=\"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch= torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#create an instance of dummygpt\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits=model(batch)\n",
    "print(\"Output shape:\",logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer normalisation is done so that to prevent the gradient magnitudes to be too large or too small\n",
    "# internal covariate shift : as training proceeds inputs to each layer can change, this delays convergence \n",
    "torch.manual_seed(123)\n",
    "batch_example=torch.randn(2,5)\n",
    "layer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out= layer(batch_example)\n",
    "print(out)\n",
    "# it is done before and after the multi-head attention  module and before final output layer\n",
    "# mean=0 variance =1\n",
    "# this is a linear layer followed by a ReLu activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var=out.var(dim=-1,keepdim=True)\n",
    "print(\"Mean:\",mean)\n",
    "print(\"Variance:\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm =(out - mean)/torch.sqrt(var)\n",
    "mean=out_norm.mean(dim = -1, keepdim = True)\n",
    "var=out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized Layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm (emb_dim=5)\n",
    "out_ln=ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1,keepdim=True)\n",
    "var=out_ln.var(dim=-1,unbiased=False,keepdim=True)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gelu activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn =FeedForward(GPT_CONFIG_124M)\n",
    "x=torch.rand(2,3,768)\n",
    "out=ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut=use_shortcut\n",
    "        self.layers=nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU())\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            layer_output=layer(x) #compute output of current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x=x+ layer_output\n",
    "            else:\n",
    "                x= layer_output\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[3,3,3,3,3,1]\n",
    "sample_input=torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut= ExampleDeepNeuralNetwork(\n",
    "    layer_sizes,use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(model, x):\n",
    "    output=model(x)#forward pass\n",
    "    target=torch.tensor([[0.]])\n",
    "\n",
    "    loss=nn.MSELoss()#calculate loss\n",
    "    loss=loss(output,target)\n",
    "\n",
    "    loss.backward()#backward pass\n",
    "\n",
    "    for name, param  in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has a gradient of {param.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has a gradient of 0.00020173584925942123\n",
      "layers.1.0.weight has a gradient of 0.00012011159560643137\n",
      "layers.2.0.weight has a gradient of 0.0007152040489017963\n",
      "layers.3.0.weight has a gradient of 0.0013988736318424344\n",
      "layers.4.0.weight has a gradient of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradient(model_without_shortcut, sample_input) # vanish gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has a gradient of 0.22169791162014008\n",
      "layers.1.0.weight has a gradient of 0.20694105327129364\n",
      "layers.2.0.weight has a gradient of 0.32896995544433594\n",
      "layers.3.0.weight has a gradient of 0.2665732204914093\n",
      "layers.4.0.weight has a gradient of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut=ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradient(model_with_shortcut,sample_input) # no vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "multihead attention-->layer normalisation -->Dropout -->Feed Forward layer -->GELU activation \n",
    "##### important components in transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257, #vocabulary\n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768, #embedding dimension\n",
    "    \"n_heads\": 12, #no of attention heads\n",
    "    \"n_layers\": 12, #no of layers\n",
    "    \"drop_rate\": 0.1, #dropout rate \n",
    "    \"qkv_bias\": False #Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the separate elements youve added\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "x=torch.rand(2,4,768)\n",
    "block=TransformerBlock(GPT_CONFIG_124M)\n",
    "output=block(x)\n",
    "print(\"Input Shape:\",x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks=nn.Sequential(\n",
    "           *[TransformerBlock(cfg) for  _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm=LayerNorm (cfg[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1=\"Every effort moves you\"\n",
    "txt2=\"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch= torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      " Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "out=model(batch)\n",
    "print(\"Input batch:\\n\",batch)\n",
    "print(\"\\n Output shape:\",out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 163009536\n"
     ]
    }
   ],
   "source": [
    "total_params=sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\",total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the next token\n",
    "### extract last vector and convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "    # idx is (batch,n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:] #if input size > context size, we select only the lasr elements the size of context size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits =model(idx_cond) #get predictions \n",
    "        \n",
    "        logits=logits[:,-1,:] # last row \n",
    "\n",
    "        # convert logits into probabilities by applying softmax\n",
    "\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        # Get the idx of the vocab entry with highest priority\n",
    "        idx_next =torch.argmax(probas, dim =-1 , keepdim =True )\n",
    "\n",
    "        # append it to the running sequence \n",
    "\n",
    "        idx= torch.cat((idx,idx_next),dim=-1)\n",
    "\n",
    "    return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor_shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context =\"Hello, I am\"\n",
    "encoded=tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor=torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor_shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output Len 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #set  model to evaluation mode\n",
    "out=generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\",out)\n",
    "print(\"Output Len\",len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text=tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text) # the output is random and messy because the model is not trained yet \n",
    "# next we will train this model for more efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "### Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model= GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # disable dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded =tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # adding batch dimension \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0) #remove batch dimension \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context =\"Every effort moves you\"\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids =generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor([[16833,3626,6100], #effort really moves\n",
    "                     [40,1107,588]]) # I really like\n",
    "\n",
    "targets=torch.tensor([[3626,6100,345], # effort moves you \n",
    "                       [1107,588,11311]])  #really likes choclate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits=model(inputs)\n",
    "probas=torch.softmax(logits,dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids=torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token IDs:\\n\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch 1:  effort moves you\n",
      "Output batch 1:  pressuring empoweredfaith\n"
     ]
    }
   ],
   "source": [
    "print(\"Target batch 1:\",token_ids_to_text(targets[0],tokenizer))\n",
    "print(\"Output batch 1:\",token_ids_to_text(token_ids[1].flatten(),tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip\n",
    "to reduce loss what we do is that we would flatten the output tensor and then flatten the target tensor and then \n",
    "check the probability of each element in the target tensor in the output tensor in the prefect case the probability should be one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx=0\n",
    "target_probas_1 =probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1:\",target_probas_1) #p11 , p12 p13\n",
    "\n",
    "text_idx=1\n",
    "target_probas_2 =probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2:\",target_probas_2) # p21, p22, p23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# concatenate them \n",
    "log_probas=torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# calculate mean of the log values\n",
    "avg_log_probas=torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# taking negative of the avg log\n",
    "neg_avg_log_probas=avg_log_probas * -1\n",
    "print(neg_avg_log_probas)# its conventional in deep learning to use negative loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Logits: torch.Size([6, 50257])\n",
      "Flattened Targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#much simpler way of doing this\n",
    "logits_flat=logits.flatten(0,1)\n",
    "targets_flat=targets.flatten()\n",
    "print(\"Flattened Logits:\",logits_flat.shape)\n",
    "print(\"Flattened Targets:\",targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# with just one line of code we get the loss\n",
    "loss= torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "# perplexity measures how well the probability distribution predicted by the model matches the actual distribution of words in the dataset\n",
    "# perplexity =exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#using the verdict to train this\n",
    "import os\n",
    "file_path=\"the-verdict.txt\"\n",
    "with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    text_data=file.read()\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "\n",
    "tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "total_char=len(text_data)\n",
    "total_tokens=len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\",total_char)\n",
    "print(\"Tokens:\",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,  #batch_size=4 no of elements in a training batch , stride=128 means the amount of movement of the window\n",
    "            max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0): # drop last is done so that the last batch of the training set may be ignored as if the batch size< 4 causes problem num_workers is the number of cpu threads that can be run simultaneously\n",
    "            tokenizer=tiktoken.get_encoding(\"gpt2\") #tokenizer initiated\n",
    "            dataset=GPTDatasetV1(txt,tokenizer,max_length,stride) #dataset made\n",
    "            dataloader= DataLoader( #create dataloader\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                drop_last=drop_last,   #this dataloaderV1 and then from getitems it gives out a input output\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "            return dataloader\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # we have to group by the number of heads\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "#combine the separate elements youve added\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)\n",
    "        val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "    # idx is (batch,n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:] #if input size > context size, we select only the lasr elements the size of context size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits =model(idx_cond) #get predictions \n",
    "        \n",
    "        logits=logits[:,-1,:] # last row \n",
    "\n",
    "        # convert logits into probabilities by applying softmax\n",
    "\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        # Get the idx of the vocab entry with highest priority\n",
    "        idx_next =torch.argmax(probas, dim =-1 , keepdim =True )\n",
    "\n",
    "        # append it to the running sequence \n",
    "\n",
    "        idx= torch.cat((idx,idx_next),dim=-1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded =tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # adding batch dimension \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0) #remove batch dimension \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context =\"Every effort moves you\"\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids =generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
    "    model.eval()\n",
    "    context_size=model.pos_emb.weight.shape[0]\n",
    "    encoded=text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids=generate_text_simple(\n",
    "            model=model,idx=encoded,\n",
    "            max_new_tokens=50,context_size=context_size\n",
    "        )\n",
    "    decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \"))\n",
    "    model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
    "    train_losses,val_losses, track_tokens_seen =[],[],[]\n",
    "    tokens_seen,global_step=0,-1\n",
    "\n",
    "    #Main training loop \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()# set model to training mode\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            loss.backward()#calculate loss gradients\n",
    "            optimizer.step()# update model weights using loss gradients\n",
    "            tokens_seen+=input_batch.numel() #returns total number of tokens\n",
    "            global_step+=1\n",
    "\n",
    "            #evaluation\n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss,val_loss=evaluate_model(\n",
    "                    model,train_loader,val_loader,device,eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "                      f\"Train Loss {train_loss:.3f}, Val Loss {val_loss:.3f}\")\n",
    "        # print sample after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model,tokenizer,device,start_context\n",
    "        )\n",
    "    return train_losses,val_losses,track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):Train Loss 9.783, Val Loss 9.927\n",
      "Ep 1 (Step 000005):Train Loss 7.985, Val Loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):Train Loss 6.753, Val Loss 7.048\n",
      "Ep 2 (Step 000015):Train Loss 6.114, Val Loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020):Train Loss 5.525, Val Loss 6.490\n",
      "Ep 3 (Step 000025):Train Loss 5.324, Val Loss 6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030):Train Loss 4.761, Val Loss 6.360\n",
      "Ep 4 (Step 000035):Train Loss 4.461, Val Loss 6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040):Train Loss 3.833, Val Loss 6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045):Train Loss 3.352, Val Loss 6.139\n",
      "Ep 6 (Step 000050):Train Loss 2.861, Val Loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055):Train Loss 2.347, Val Loss 6.138\n",
      "Ep 7 (Step 000060):Train Loss 2.084, Val Loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065):Train Loss 1.521, Val Loss 6.176\n",
      "Ep 8 (Step 000070):Train Loss 1.272, Val Loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075):Train Loss 1.000, Val Loss 6.277\n",
      "Ep 9 (Step 000080):Train Loss 0.718, Val Loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085):Train Loss 0.506, Val Loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n",
      "Training Completed in 19.73 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\n",
    "\n",
    "num_epochs=10\n",
    "train_losses,val_losses,tokens_seen= train_model_simple(\n",
    "    model,train_loader,val_loader,optimizer,device,num_epochs=num_epochs,\n",
    "    eval_freq=5,eval_iter=5,start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time=time.time()\n",
    "execution_time_minutes=(end_time - start_time)/60\n",
    "print(f\"Training Completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats your name? It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I saw that, a fashionable painter--that I was not\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(\n",
    "            model,tokenizer,device,\"whats your name?\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in ./venv/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Strats to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids=generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategy 1: Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use multinomial sampling\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "next_token_logits2 = next_token_logits/0.1\n",
    "\n",
    "next_token_logits3 = next_token_logits/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8530e-10, 3.5189e-26, 2.6890e-38, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
      "        2.9718e-38, 9.0133e-03, 2.8514e-22])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits2, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits3, dim=0)\n",
    "\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
      "        1.0120e-04, 3.5758e-01, 4.0122e-03])\n",
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(next_token_id)\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "##Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Strategy 2: Top-k Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k=3\n",
    "top_logits,top_pos=torch.topk(next_token_logits,top_k)\n",
    "print(\"Top logits:\",top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits=torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas=torch.softmax(new_logits,dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging topk and temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx_cond)\n",
    "        logits=logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            # keep topk values\n",
    "            top_logits,_ = torch.topk(logits,top_k)\n",
    "            min_val=top_logits[:,-1]\n",
    "            logits=torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
    "\n",
    "\n",
    "        #apply temp scaling\n",
    "        if temperature >0.0:\n",
    "            logits=logits/temperature\n",
    "\n",
    "            # softmax\n",
    "            probs=torch.softmax(logits,dim=-1) # (batch_size,context_len)\n",
    "\n",
    "            #sample from distribution \n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "        \n",
    "        #otherwise same as before\n",
    "        else:\n",
    "            idx_next=torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        \n",
    "\n",
    "        if idx_next== eos_id:\n",
    "            break\n",
    "        \n",
    "        # append sampled text to the sequence\n",
    "        idx=torch.cat((idx,idx_next),dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids=generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(),\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43641/2906731100.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\n",
    "torch.save({\n",
    "    \"model_state_dict\":model.state_dict(),\n",
    "    \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43641/1242810220.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint=torch.load(\"model_and_optimizer.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=torch.load(\"model_and_optimizer.pth\")\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=5e-4,weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained openAI weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 22:03:36.675853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753374816.824105   43641 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753374816.869099   43641 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753374817.134710   43641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753374817.134760   43641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753374817.134767   43641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753374817.134773   43641 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-24 22:03:37.173906: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm \n",
    "print(tf.__version__)\n",
    "print(tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 22:03:53.031045: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "settings,params=download_and_load_gpt2(model_size=\"124M\",models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "NEW_CONFIG=GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update({\"context_length\":1024,\"qkv_bias\":True})\n",
    "gpt=GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign (left,right):\n",
    "    if left.shape !=right.shape:\n",
    "        raise ValueError(f\"Shape Mismatch. Left:{left.shape} , Right:{right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight=assign(gpt.pos_emb.weight,params['wpe'])\n",
    "    gpt.tok_emb.weight=assign(gpt.tok_emb.weight,params['wte'])\n",
    "    for b in range (len(params[\"blocks\"])):\n",
    "        q_w,k_w,v_w=np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3,axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight,q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight,k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight,v_w.T\n",
    "        )\n",
    "\n",
    "        q_b,k_b,v_b=np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3,axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias,q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias,k_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias,v_b\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight=assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias=assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale=assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift=assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale=assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift=assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    gpt.final_norm.scale=assign(gpt.final_norm.scale,params[\"g\"])\n",
    "    gpt.final_norm.shift=assign(gpt.final_norm.shift,params[\"b\"])\n",
    "    gpt.out_head.weight=assign(gpt.out_head.weight,params[\"wte\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m      2\u001b[39m token_ids=generate(\n\u001b[32m      3\u001b[39m     model=gpt,\n\u001b[32m      4\u001b[39m     idx=text_to_token_ids(\u001b[33m\"\u001b[39m\u001b[33mEvery effort moves you\u001b[39m\u001b[33m\"\u001b[39m,tokenizer).to(device),\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     temperature=\u001b[32m1\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,token_ids_to_text(token_ids,tokenizer))\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids=generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "instruction based classification based finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the code before\n",
    "its anoying scrolling up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.4%\n"
     ]
    }
   ],
   "source": [
    "# Add to a cell and run after restart\n",
    "%reset -f\n",
    "import psutil\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.3%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # we have to group by the number of heads\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "#combine the separate elements youve added\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "import torch.nn as nn \n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)\n",
    "        val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss,val_loss\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "    # idx is (batch,n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:] #if input size > context size, we select only the lasr elements the size of context size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits =model(idx_cond) #get predictions \n",
    "        \n",
    "        logits=logits[:,-1,:] # last row \n",
    "\n",
    "        # convert logits into probabilities by applying softmax\n",
    "\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        # Get the idx of the vocab entry with highest priority\n",
    "        idx_next =torch.argmax(probas, dim =-1 , keepdim =True )\n",
    "\n",
    "        # append it to the running sequence \n",
    "\n",
    "        idx= torch.cat((idx,idx_next),dim=-1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded =tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # adding batch dimension \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0) #remove batch dimension \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
    "    model.eval()\n",
    "    context_size=model.pos_emb.weight.shape[0]\n",
    "    encoded=text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids=generate_text_simple(\n",
    "            model=model,idx=encoded,\n",
    "            max_new_tokens=50,context_size=context_size\n",
    "        )\n",
    "    decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \"))\n",
    "    model.train\n",
    "\n",
    "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
    "    train_losses,val_losses, track_tokens_seen =[],[],[]\n",
    "    tokens_seen,global_step=0,-1\n",
    "\n",
    "    #Main training loop \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()# set model to training mode\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            loss.backward()#calculate loss gradients\n",
    "            optimizer.step()# update model weights using loss gradients\n",
    "            tokens_seen+=input_batch.numel() #returns total number of tokens\n",
    "            global_step+=1\n",
    "\n",
    "            #evaluation\n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss,val_loss=evaluate_model(\n",
    "                    model,train_loader,val_loader,device,eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "                      f\"Train Loss {train_loss:.3f}, Val Loss {val_loss:.3f}\")\n",
    "        # print sample after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model,tokenizer,device,start_context\n",
    "        )\n",
    "    return train_losses,val_losses,track_tokens_seen\n",
    "\n",
    "def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx_cond)\n",
    "        logits=logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            # keep topk values\n",
    "            top_logits,_ = torch.topk(logits,top_k)\n",
    "            min_val=top_logits[:,-1]\n",
    "            logits=torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
    "\n",
    "\n",
    "        #apply temp scaling\n",
    "        if temperature >0.0:\n",
    "            logits=logits/temperature\n",
    "\n",
    "            # softmax\n",
    "            probs=torch.softmax(logits,dim=-1) # (batch_size,context_len)\n",
    "\n",
    "            #sample from distribution \n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "        \n",
    "        #otherwise same as before\n",
    "        else:\n",
    "            idx_next=torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        \n",
    "\n",
    "        if idx_next== eos_id:\n",
    "            break\n",
    "        \n",
    "        # append sampled text to the sequence\n",
    "        idx=torch.cat((idx,idx_next),dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def assign (left,right):\n",
    "    if left.shape !=right.shape:\n",
    "        raise ValueError(f\"Shape Mismatch. Left:{left.shape} , Right:{right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt,params):\n",
    "    gpt.pos_emb.weight=assign(gpt.pos_emb.weight,params['wpe'])\n",
    "    gpt.tok_emb.weight=assign(gpt.tok_emb.weight,params['wte'])\n",
    "    for b in range (len(params[\"blocks\"])):\n",
    "        q_w,k_w,v_w=np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3,axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight,q_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight,k_w.T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.weight=assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight,v_w.T\n",
    "        )\n",
    "\n",
    "        q_b,k_b,v_b=np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3,axis=-1\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_query.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias,q_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_key.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias,k_b\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.W_value.bias=assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias,v_b\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight=assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].att.out_proj.bias=assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T\n",
    "        )\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias=assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale=assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm1.shift=assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.scale=assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"]\n",
    "        )\n",
    "        gpt.trf_blocks[b].norm2.shift=assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    gpt.final_norm.scale=assign(gpt.final_norm.scale,params[\"g\"])\n",
    "    gpt.final_norm.shift=assign(gpt.final_norm.shift,params[\"b\"])\n",
    "    gpt.out_head.weight=assign(gpt.out_head.weight,params[\"wte\"])\n",
    "\n",
    "\n",
    "import psutil\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Memory usage: 44.1%\n"
     ]
    }
   ],
   "source": [
    "# parameter efficient finetuning (PEFT) like Lora and QLora\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import ssl\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "import psutil\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.2%\n"
     ]
    }
   ],
   "source": [
    "#Converting the instructions to the alpaca format\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n",
      "Memory usage: 44.1%\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)\n",
    "\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.1%\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n",
      "Memory usage: 44.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.3%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n",
      "Memory usage: 44.6%\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.6%\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Memory usage: 44.6%\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.6%\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.4%\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n",
      "Memory usage: 44.3%\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Memory usage: 44.2%\n"
     ]
    }
   ],
   "source": [
    "# Masking the target tokrn ids\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 44.6%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 1 #originally 8 but the system is not able to handle it\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 76]) torch.Size([1, 76])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 73]) torch.Size([1, 73])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 72]) torch.Size([1, 72])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 80]) torch.Size([1, 80])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 77]) torch.Size([1, 77])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 42]) torch.Size([1, 42])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 79]) torch.Size([1, 79])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 80]) torch.Size([1, 80])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 42]) torch.Size([1, 42])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 76]) torch.Size([1, 76])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 91]) torch.Size([1, 91])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 89]) torch.Size([1, 89])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 88]) torch.Size([1, 88])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 74]) torch.Size([1, 74])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 76]) torch.Size([1, 76])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 40]) torch.Size([1, 40])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 42]) torch.Size([1, 42])\n",
      "torch.Size([1, 74]) torch.Size([1, 74])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 80]) torch.Size([1, 80])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 87]) torch.Size([1, 87])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 72]) torch.Size([1, 72])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 72]) torch.Size([1, 72])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 72]) torch.Size([1, 72])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 42]) torch.Size([1, 42])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 74]) torch.Size([1, 74])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 73]) torch.Size([1, 73])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 80]) torch.Size([1, 80])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 70]) torch.Size([1, 70])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 91]) torch.Size([1, 91])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 71]) torch.Size([1, 71])\n",
      "torch.Size([1, 80]) torch.Size([1, 80])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 47]) torch.Size([1, 47])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 81]) torch.Size([1, 81])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 74]) torch.Size([1, 74])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 82]) torch.Size([1, 82])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 77]) torch.Size([1, 77])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 42]) torch.Size([1, 42])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 68]) torch.Size([1, 68])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 63]) torch.Size([1, 63])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 77]) torch.Size([1, 77])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 91]) torch.Size([1, 91])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 43]) torch.Size([1, 43])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 75]) torch.Size([1, 75])\n",
      "torch.Size([1, 54]) torch.Size([1, 54])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 41]) torch.Size([1, 41])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 67]) torch.Size([1, 67])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 78]) torch.Size([1, 78])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 76]) torch.Size([1, 76])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 51]) torch.Size([1, 51])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 55]) torch.Size([1, 55])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 83]) torch.Size([1, 83])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 66]) torch.Size([1, 66])\n",
      "torch.Size([1, 62]) torch.Size([1, 62])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 50]) torch.Size([1, 50])\n",
      "torch.Size([1, 45]) torch.Size([1, 45])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n",
      "torch.Size([1, 52]) torch.Size([1, 52])\n",
      "torch.Size([1, 44]) torch.Size([1, 44])\n",
      "torch.Size([1, 73]) torch.Size([1, 73])\n",
      "torch.Size([1, 48]) torch.Size([1, 48])\n",
      "torch.Size([1, 74]) torch.Size([1, 74])\n",
      "torch.Size([1, 57]) torch.Size([1, 57])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 60]) torch.Size([1, 60])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 56]) torch.Size([1, 56])\n",
      "torch.Size([1, 46]) torch.Size([1, 46])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 69]) torch.Size([1, 69])\n",
      "torch.Size([1, 58]) torch.Size([1, 58])\n",
      "torch.Size([1, 61]) torch.Size([1, 61])\n",
      "torch.Size([1, 49]) torch.Size([1, 49])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 59]) torch.Size([1, 59])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1, 72]) torch.Size([1, 72])\n",
      "torch.Size([1, 53]) torch.Size([1, 53])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "Memory usage: 44.8%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Memory usage: 75.8%\n"
     ]
    }
   ],
   "source": [
    "from gpt_download3 import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\":1024, #original value is 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n",
      "Memory usage: 74.4%\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 75.3%\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "print(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.015831232070923\n",
      "Validation loss: 4.1442711353302\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.250, Val loss 3.279\n",
      "Ep 1 (Step 000005): Train loss 1.987, Val loss 1.818\n",
      "Ep 1 (Step 000010): Train loss 1.018, Val loss 1.371\n",
      "Ep 1 (Step 000015): Train loss 1.320, Val loss 1.285\n",
      "Ep 1 (Step 000020): Train loss 0.794, Val loss 1.220\n",
      "Ep 1 (Step 000025): Train loss 0.878, Val loss 1.163\n",
      "Ep 1 (Step 000030): Train loss 0.978, Val loss 1.136\n",
      "Ep 1 (Step 000035): Train loss 1.111, Val loss 1.125\n",
      "Ep 1 (Step 000040): Train loss 1.158, Val loss 1.076\n",
      "Ep 1 (Step 000045): Train loss 0.750, Val loss 1.048\n",
      "Ep 1 (Step 000050): Train loss 1.156, Val loss 1.019\n",
      "Ep 1 (Step 000055): Train loss 1.190, Val loss 1.012\n",
      "Ep 1 (Step 000060): Train loss 1.122, Val loss 1.035\n",
      "Ep 1 (Step 000065): Train loss 0.934, Val loss 1.034\n",
      "Ep 1 (Step 000070): Train loss 0.625, Val loss 1.044\n",
      "Ep 1 (Step 000075): Train loss 0.685, Val loss 1.042\n",
      "Ep 1 (Step 000080): Train loss 0.910, Val loss 1.053\n",
      "Ep 1 (Step 000085): Train loss 0.699, Val loss 1.077\n",
      "Ep 1 (Step 000090): Train loss 0.860, Val loss 1.090\n",
      "Ep 1 (Step 000095): Train loss 0.783, Val loss 1.089\n",
      "Ep 1 (Step 000100): Train loss 0.735, Val loss 1.058\n",
      "Ep 1 (Step 000105): Train loss 0.749, Val loss 1.044\n",
      "Ep 1 (Step 000110): Train loss 1.075, Val loss 1.037\n",
      "Ep 1 (Step 000115): Train loss 0.771, Val loss 1.037\n",
      "Ep 1 (Step 000120): Train loss 1.275, Val loss 1.047\n",
      "Ep 1 (Step 000125): Train loss 0.712, Val loss 1.047\n",
      "Ep 1 (Step 000130): Train loss 0.838, Val loss 1.062\n",
      "Ep 1 (Step 000135): Train loss 0.737, Val loss 1.077\n",
      "Ep 1 (Step 000140): Train loss 0.944, Val loss 1.075\n",
      "Ep 1 (Step 000145): Train loss 0.862, Val loss 1.061\n",
      "Ep 1 (Step 000150): Train loss 0.673, Val loss 1.055\n",
      "Ep 1 (Step 000155): Train loss 0.822, Val loss 1.052\n",
      "Ep 1 (Step 000160): Train loss 0.690, Val loss 1.057\n",
      "Ep 1 (Step 000165): Train loss 1.175, Val loss 1.051\n",
      "Ep 1 (Step 000170): Train loss 0.747, Val loss 1.030\n",
      "Ep 1 (Step 000175): Train loss 1.111, Val loss 1.026\n",
      "Ep 1 (Step 000180): Train loss 0.674, Val loss 1.024\n",
      "Ep 1 (Step 000185): Train loss 0.835, Val loss 1.020\n",
      "Ep 1 (Step 000190): Train loss 0.967, Val loss 1.028\n",
      "Ep 1 (Step 000195): Train loss 0.766, Val loss 1.036\n",
      "Ep 1 (Step 000200): Train loss 1.096, Val loss 1.026\n",
      "Ep 1 (Step 000205): Train loss 1.067, Val loss 1.035\n",
      "Ep 1 (Step 000210): Train loss 1.123, Val loss 1.035\n",
      "Ep 1 (Step 000215): Train loss 0.562, Val loss 1.021\n",
      "Ep 1 (Step 000220): Train loss 0.526, Val loss 1.007\n",
      "Ep 1 (Step 000225): Train loss 0.474, Val loss 0.974\n",
      "Ep 1 (Step 000230): Train loss 0.765, Val loss 0.959\n",
      "Ep 1 (Step 000235): Train loss 0.924, Val loss 0.951\n",
      "Ep 1 (Step 000240): Train loss 0.888, Val loss 0.946\n",
      "Ep 1 (Step 000245): Train loss 0.892, Val loss 0.945\n",
      "Ep 1 (Step 000250): Train loss 1.011, Val loss 0.941\n",
      "Ep 1 (Step 000255): Train loss 0.519, Val loss 0.940\n",
      "Ep 1 (Step 000260): Train loss 0.897, Val loss 0.939\n",
      "Ep 1 (Step 000265): Train loss 0.531, Val loss 0.945\n",
      "Ep 1 (Step 000270): Train loss 0.752, Val loss 0.951\n",
      "Ep 1 (Step 000275): Train loss 0.865, Val loss 0.961\n",
      "Ep 1 (Step 000280): Train loss 0.743, Val loss 0.976\n",
      "Ep 1 (Step 000285): Train loss 0.830, Val loss 0.979\n",
      "Ep 1 (Step 000290): Train loss 0.756, Val loss 0.996\n",
      "Ep 1 (Step 000295): Train loss 0.870, Val loss 1.005\n",
      "Ep 1 (Step 000300): Train loss 0.923, Val loss 1.003\n",
      "Ep 1 (Step 000305): Train loss 0.783, Val loss 0.997\n",
      "Ep 1 (Step 000310): Train loss 0.677, Val loss 0.984\n",
      "Ep 1 (Step 000315): Train loss 0.643, Val loss 0.978\n",
      "Ep 1 (Step 000320): Train loss 0.925, Val loss 0.971\n",
      "Ep 1 (Step 000325): Train loss 0.695, Val loss 0.969\n",
      "Ep 1 (Step 000330): Train loss 0.573, Val loss 0.972\n",
      "Ep 1 (Step 000335): Train loss 0.661, Val loss 0.970\n",
      "Ep 1 (Step 000340): Train loss 1.280, Val loss 0.973\n",
      "Ep 1 (Step 000345): Train loss 0.903, Val loss 0.981\n",
      "Ep 1 (Step 000350): Train loss 0.614, Val loss 1.000\n",
      "Ep 1 (Step 000355): Train loss 0.574, Val loss 0.990\n",
      "Ep 1 (Step 000360): Train loss 0.909, Val loss 0.973\n",
      "Ep 1 (Step 000365): Train loss 0.563, Val loss 0.961\n",
      "Ep 1 (Step 000370): Train loss 0.842, Val loss 0.957\n",
      "Ep 1 (Step 000375): Train loss 0.681, Val loss 0.940\n",
      "Ep 1 (Step 000380): Train loss 0.600, Val loss 0.923\n",
      "Ep 1 (Step 000385): Train loss 0.952, Val loss 0.938\n",
      "Ep 1 (Step 000390): Train loss 0.518, Val loss 0.942\n",
      "Ep 1 (Step 000395): Train loss 0.846, Val loss 0.942\n",
      "Ep 1 (Step 000400): Train loss 0.657, Val loss 0.934\n",
      "Ep 1 (Step 000405): Train loss 0.706, Val loss 0.920\n",
      "Ep 1 (Step 000410): Train loss 0.680, Val loss 0.922\n",
      "Ep 1 (Step 000415): Train loss 1.159, Val loss 0.930\n",
      "Ep 1 (Step 000420): Train loss 0.850, Val loss 0.913\n",
      "Ep 1 (Step 000425): Train loss 0.602, Val loss 0.904\n",
      "Ep 1 (Step 000430): Train loss 0.650, Val loss 0.919\n",
      "Ep 1 (Step 000435): Train loss 0.545, Val loss 0.922\n",
      "Ep 1 (Step 000440): Train loss 0.542, Val loss 0.887\n",
      "Ep 1 (Step 000445): Train loss 0.506, Val loss 0.873\n",
      "Ep 1 (Step 000450): Train loss 0.480, Val loss 0.873\n",
      "Ep 1 (Step 000455): Train loss 0.778, Val loss 0.877\n",
      "Ep 1 (Step 000460): Train loss 0.675, Val loss 0.884\n",
      "Ep 1 (Step 000465): Train loss 0.631, Val loss 0.872\n",
      "Ep 1 (Step 000470): Train loss 0.433, Val loss 0.866\n",
      "Ep 1 (Step 000475): Train loss 0.624, Val loss 0.868\n",
      "Ep 1 (Step 000480): Train loss 0.852, Val loss 0.872\n",
      "Ep 1 (Step 000485): Train loss 0.511, Val loss 0.883\n",
      "Ep 1 (Step 000490): Train loss 0.617, Val loss 0.893\n",
      "Ep 1 (Step 000495): Train loss 0.521, Val loss 0.896\n",
      "Ep 1 (Step 000500): Train loss 0.635, Val loss 0.899\n",
      "Ep 1 (Step 000505): Train loss 1.006, Val loss 0.902\n",
      "Ep 1 (Step 000510): Train loss 0.619, Val loss 0.899\n",
      "Ep 1 (Step 000515): Train loss 0.501, Val loss 0.899\n",
      "Ep 1 (Step 000520): Train loss 0.638, Val loss 0.897\n",
      "Ep 1 (Step 000525): Train loss 0.723, Val loss 0.889\n",
      "Ep 1 (Step 000530): Train loss 0.628, Val loss 0.876\n",
      "Ep 1 (Step 000535): Train loss 0.660, Val loss 0.864\n",
      "Ep 1 (Step 000540): Train loss 0.652, Val loss 0.864\n",
      "Ep 1 (Step 000545): Train loss 0.674, Val loss 0.869\n",
      "Ep 1 (Step 000550): Train loss 0.719, Val loss 0.872\n",
      "Ep 1 (Step 000555): Train loss 0.616, Val loss 0.877\n",
      "Ep 1 (Step 000560): Train loss 0.714, Val loss 0.876\n",
      "Ep 1 (Step 000565): Train loss 0.541, Val loss 0.870\n",
      "Ep 1 (Step 000570): Train loss 0.539, Val loss 0.871\n",
      "Ep 1 (Step 000575): Train loss 0.539, Val loss 0.880\n",
      "Ep 1 (Step 000580): Train loss 0.779, Val loss 0.880\n",
      "Ep 1 (Step 000585): Train loss 0.893, Val loss 0.877\n",
      "Ep 1 (Step 000590): Train loss 0.529, Val loss 0.871\n",
      "Ep 1 (Step 000595): Train loss 0.725, Val loss 0.867\n",
      "Ep 1 (Step 000600): Train loss 0.509, Val loss 0.883\n",
      "Ep 1 (Step 000605): Train loss 0.496, Val loss 0.900\n",
      "Ep 1 (Step 000610): Train loss 0.773, Val loss 0.908\n",
      "Ep 1 (Step 000615): Train loss 0.606, Val loss 0.910\n",
      "Ep 1 (Step 000620): Train loss 0.848, Val loss 0.876\n",
      "Ep 1 (Step 000625): Train loss 0.515, Val loss 0.875\n",
      "Ep 1 (Step 000630): Train loss 0.969, Val loss 0.886\n",
      "Ep 1 (Step 000635): Train loss 0.741, Val loss 0.893\n",
      "Ep 1 (Step 000640): Train loss 0.534, Val loss 0.870\n",
      "Ep 1 (Step 000645): Train loss 0.533, Val loss 0.869\n",
      "Ep 1 (Step 000650): Train loss 0.680, Val loss 0.866\n",
      "Ep 1 (Step 000655): Train loss 0.463, Val loss 0.878\n",
      "Ep 1 (Step 000660): Train loss 0.565, Val loss 0.889\n",
      "Ep 1 (Step 000665): Train loss 0.685, Val loss 0.891\n",
      "Ep 1 (Step 000670): Train loss 0.638, Val loss 0.888\n",
      "Ep 1 (Step 000675): Train loss 0.422, Val loss 0.888\n",
      "Ep 1 (Step 000680): Train loss 0.570, Val loss 0.882\n",
      "Ep 1 (Step 000685): Train loss 0.354, Val loss 0.877\n",
      "Ep 1 (Step 000690): Train loss 0.627, Val loss 0.860\n",
      "Ep 1 (Step 000695): Train loss 0.411, Val loss 0.822\n",
      "Ep 1 (Step 000700): Train loss 0.543, Val loss 0.796\n",
      "Ep 1 (Step 000705): Train loss 0.543, Val loss 0.769\n",
      "Ep 1 (Step 000710): Train loss 0.641, Val loss 0.746\n",
      "Ep 1 (Step 000715): Train loss 0.512, Val loss 0.741\n",
      "Ep 1 (Step 000720): Train loss 0.341, Val loss 0.751\n",
      "Ep 1 (Step 000725): Train loss 0.615, Val loss 0.763\n",
      "Ep 1 (Step 000730): Train loss 0.642, Val loss 0.775\n",
      "Ep 1 (Step 000735): Train loss 0.507, Val loss 0.777\n",
      "Ep 1 (Step 000740): Train loss 0.475, Val loss 0.779\n",
      "Ep 1 (Step 000745): Train loss 0.497, Val loss 0.778\n",
      "Ep 1 (Step 000750): Train loss 0.500, Val loss 0.771\n",
      "Ep 1 (Step 000755): Train loss 0.662, Val loss 0.781\n",
      "Ep 1 (Step 000760): Train loss 0.812, Val loss 0.792\n",
      "Ep 1 (Step 000765): Train loss 0.414, Val loss 0.806\n",
      "Ep 1 (Step 000770): Train loss 0.639, Val loss 0.804\n",
      "Ep 1 (Step 000775): Train loss 0.529, Val loss 0.789\n",
      "Ep 1 (Step 000780): Train loss 0.594, Val loss 0.779\n",
      "Ep 1 (Step 000785): Train loss 0.415, Val loss 0.770\n",
      "Ep 1 (Step 000790): Train loss 0.731, Val loss 0.770\n",
      "Ep 1 (Step 000795): Train loss 0.529, Val loss 0.774\n",
      "Ep 1 (Step 000800): Train loss 0.645, Val loss 0.777\n",
      "Ep 1 (Step 000805): Train loss 0.660, Val loss 0.775\n",
      "Ep 1 (Step 000810): Train loss 0.612, Val loss 0.769\n",
      "Ep 1 (Step 000815): Train loss 0.447, Val loss 0.760\n",
      "Ep 1 (Step 000820): Train loss 0.796, Val loss 0.755\n",
      "Ep 1 (Step 000825): Train loss 0.563, Val loss 0.759\n",
      "Ep 1 (Step 000830): Train loss 0.511, Val loss 0.764\n",
      "Ep 1 (Step 000835): Train loss 0.567, Val loss 0.767\n",
      "Ep 1 (Step 000840): Train loss 0.471, Val loss 0.767\n",
      "Ep 1 (Step 000845): Train loss 0.427, Val loss 0.761\n",
      "Ep 1 (Step 000850): Train loss 0.522, Val loss 0.763\n",
      "Ep 1 (Step 000855): Train loss 0.467, Val loss 0.760\n",
      "Ep 1 (Step 000860): Train loss 0.676, Val loss 0.761\n",
      "Ep 1 (Step 000865): Train loss 0.472, Val loss 0.765\n",
      "Ep 1 (Step 000870): Train loss 0.459, Val loss 0.772\n",
      "Ep 1 (Step 000875): Train loss 0.574, Val loss 0.790\n",
      "Ep 1 (Step 000880): Train loss 0.445, Val loss 0.774\n",
      "Ep 1 (Step 000885): Train loss 0.485, Val loss 0.754\n",
      "Ep 1 (Step 000890): Train loss 0.689, Val loss 0.740\n",
      "Ep 1 (Step 000895): Train loss 0.452, Val loss 0.724\n",
      "Ep 1 (Step 000900): Train loss 0.444, Val loss 0.711\n",
      "Ep 1 (Step 000905): Train loss 0.450, Val loss 0.706\n",
      "Ep 1 (Step 000910): Train loss 0.558, Val loss 0.698\n",
      "Ep 1 (Step 000915): Train loss 0.528, Val loss 0.680\n",
      "Ep 1 (Step 000920): Train loss 0.512, Val loss 0.680\n",
      "Ep 1 (Step 000925): Train loss 0.471, Val loss 0.679\n",
      "Ep 1 (Step 000930): Train loss 0.559, Val loss 0.680\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal every day is cooked by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The chef prepares the meal every day.\n",
      "Training completed in 66.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a superstorm.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Robert Frost.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
