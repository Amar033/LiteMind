{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and reading in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open (\"the-verdict.txt\",'r',encoding=\"utf-8\") as f :\n",
    "    raw_text=f.read()\n",
    "print(\"Total number of characters:\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed= re.split(r'(,.:;?!\"()\\']|--|\\s)',text)\n",
    "        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed=[\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<|unk|>'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer= SimpleTokenizerV2(vocab)\n\u001b[32m      2\u001b[39m text=\u001b[33m\"\u001b[39m\u001b[33mHello It\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the last time I am going to see you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ids=\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimpleTokenizerV2.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      8\u001b[39m preprocessed=[item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m      9\u001b[39m preprocessed=[\n\u001b[32m     10\u001b[39m     item \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.str_to_int \n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m<|unk|>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed\n\u001b[32m     12\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids=[\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: '<|unk|>'"
     ]
    }
   ],
   "source": [
    "tokenizer= SimpleTokenizerV2(vocab)\n",
    "text=\"Hello It's the last time I am going to see you\"\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|unk|> It' s the last time I am going to see you\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bytepair Encoding simillar to Gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 23748, 703, 389, 345, 30, 220, 50256, 383, 995, 318, 257, 4950, 1295]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "text=\"Hi hello how are you? <|endoftext|> The world is a beautiful place\"\n",
    "integers=tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi hello how are you? <|endoftext|> The world is a beautiful place\n"
     ]
    }
   ],
   "source": [
    "string=tokenizer.decode(integers)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [568, 115, 1066, 727]\n",
      "y:      [115, 1066, 727, 988]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "#length of the input\n",
    "# model looks at the 4 words and then predict the next word\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[568] ----> 115\n",
      "[568, 115] ----> 1066\n",
      "[568, 115, 1066] ----> 727\n",
      "[568, 115, 1066, 727] ----> 988\n"
     ]
    }
   ],
   "source": [
    "for  i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "    print(context,\"---->\",desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ----> a\n",
      "in a ----> villa\n",
      "in a villa ----> on\n",
      "in a villa on ----> the\n"
     ]
    }
   ],
   "source": [
    "for  i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "    print(tokenizer.decode(context),\"---->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[] #input chunk \n",
    "        self.target_ids=[] #output chunk \n",
    "\n",
    "        token_ids=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids)>max_length,\"Number of token_ids must be atleast equal to max_length + 1\"\n",
    "        #sliding window \n",
    "        for i in range(0,len(token_ids)-max_length,stride): \n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            output_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx] #mapping dataset map each input to corresponding output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,  #batch_size=4 no of elements in a training batch , stride=128 means the amount of movement of the window\n",
    "            max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0): # drop last is done so that the last batch of the training set may be ignored as if the batch size< 4 causes problem num_workers is the number of cpu threads that can be run simultaneously\n",
    "            tokenizer=tiktoken.get_encoding(\"gpt2\") #tokenizer initiated\n",
    "            dataset=GPTDatasetV1(txt,tokenizer,max_length,stride) #dataset made\n",
    "            dataloader= DataLoader( #create dataloader\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                drop_last=drop_last,   #this dataloaderV1 and then from getitems it gives out a input output\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "            return dataloader\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf_8\") as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "dataloader=create_dataloader_v1(\n",
    "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
    ")\n",
    "data_iter=iter(dataloader)\n",
    "first_batch=next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch=next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=4,stride=4,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"Targets:\\n\",targets)\n",
    "# each of the tensor contains 4 tokens since the max_length is also 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1]) #consider they are token ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123) #generate random in any device\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) #embedding layer of 6*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight) #embedding layer is an efficient way to implement one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3]))) #convert token id 3 into a vector of 3 dimension and print it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding=token_embedding_layer(inputs)\n",
    "# print(token_embedding) # 50257 *256 50257 tokens and 256 dimensions\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length #another embedding layer because gpt 2 uses position embeddings\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embedding=pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embedding.shape)\n",
    "print(pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#input embedding will be posn embedding + token embedding \n",
    "input_embedding=pos_embedding+token_embedding\n",
    "print(input_embedding.shape)\n",
    "# print(input_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have completed preprocessing, tokenization , and token embeddings with positional embeddings\n",
    "#simple self attention mechanism without trainable weights\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "#normalising\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#softmax normalization\n",
    "# e^x /(e^x1+....e^xt)\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)# summation of eac row   \n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#pytorch softmax because naive softmax can have underflow or overflow\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)# final context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#this is for just 'journey' now lets do this for each text\n",
    "attn_scores = torch.empty(len(inputs), len(inputs))\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#the above operation works but is very complex and not scalable\n",
    "#but according to linear algebra\n",
    "attn_score = inputs @ inputs.T\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs= attn_weights @ inputs\n",
    "print(all_context_vecs) #literally the same scaling operation as before(write down and check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we can do this then why do we need trainable weights? \n",
    "because there is meaning in the current sentence, like here there is almost no relation between one and journey but consider there might be in this specific context, so to represent that we need to have trainable weights\n",
    "\n",
    "##### eg: the cat sat on the mat because it was warm -> if warm is the query then mat needs to have a higher attention score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing self-attention with trainable weights\n",
    "\n",
    "\n",
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "\n",
    "#in gpt like models the input and output dimensions are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "#initialize weight matrices \n",
    "torch.manual_seed(123)\n",
    "# we need to learn how to convert input vectors into key query and value vectors\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#when using weight matrices for model training \n",
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_2 = x_2 @ W_query # x_2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "query.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "queries= inputs @ W_query\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "print(\"query.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "#calcultating attention score\n",
    "\n",
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2= query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores =  queries @ keys.T #all queries with attention \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# now normalisation \n",
    "# attention weights- they should sum up to 1\n",
    "# first scale by root d-keys and then softmax\n",
    "#why dividibng with root of d_k? it gives a stable output\n",
    "#relared to variance\n",
    "d_k= keys.shape[-1]\n",
    "attn_weights_2=torch.softmax(attn_scores_2/d_k**0.5,dim =-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a self attention python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_V1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value=nn.Parameter(torch.rand(d_in,d_out))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        keys=x @ self.W_key\n",
    "        queries=x @ self.W_query\n",
    "        values=x @ self.W_value\n",
    "        attn_score=queries @ keys.T\n",
    "        attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vec= attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1=SelfAttention_V1(d_in,d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) #stable\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out) #weight initialization is more sophisticates\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# mask out future tokens\n",
    "# we mask out the attention weights above the diagonal and then we normalise the attention weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores= queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim =-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# now we generate a mask \n",
    "# so we basically have the output(attention weights) and then mask it then normalize rows\n",
    "context_length= attn_scores.shape[0]\n",
    "mask_simple=torch.tril(torch.ones(context_length,context_length))# tril creates a lower triangular matrix\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple= attn_weights * mask_simple #canceling influence of future tokens (data leakage) this is bad\n",
    "print(masked_simple) #now normalise it to have each row sum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums=masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums \n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) #fill the positive values with -inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#code should be able to handle batches\n",
    "# we will simulate batch inputs for now we are using 2 inputs with 6 tokens and each token has embedding dimension of 3\n",
    "batch = torch.stack((inputs, inputs),dim=0)\n",
    "print(batch.shape)\n",
    "# the causal attention class will be simillar to self attention but we now have dropout and causal masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "ca = CausalAttention(d_in,d_out,context_length,0.0)\n",
    "context_vecs=ca(batch)\n",
    "print(\"Context_vecs.shape:\",context_vecs.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs),dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1(one type of mutihead attention)\n",
    "# divide attention mechanism into \"multiple heads\" each operating independently \n",
    "# instead of a single key,query and value matrix we have multiple (for now lets just say 2)\n",
    "# hence we get 2 attention weights and then 2 context vectors that will be then combined to a single vector\n",
    "\n",
    "# consider each context vec to have 6 x 2 dimension so the combined context vector will have the dimension 6 x 4\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "# this current system works well but the single heads are processed sequentially and then aggregated, we can improve them by making this in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multihead attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # we have to group by the number of heads\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Context_vecs Shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "inputs=torch.tensor(\n",
    "    [[0.43,0.15,0.89,0.55,0.87,0.66],\n",
    "     [0.57,0.85,0.64,0.22,0.58,0.33],\n",
    "     [0.77,0.25,0.10,0.05,0.80,0.55]]\n",
    ")\n",
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "print(batch.shape)\n",
    "batch_size,context_length,d_in=batch.shape\n",
    "d_out=6\n",
    "mha=MultiHeadAttention(d_in,d_out,context_length,0.0,num_heads=2)\n",
    "context_vecs=mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context_vecs Shape:\",context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GPT model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to make a placeholder for gpt model\n",
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257, #vocabulary\n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768, #embedding dimension\n",
    "    \"n_heads\": 12, #no of attention heads\n",
    "    \"n_layers\": 12, #no of layers\n",
    "    \"drop_rate\": 0.1, #dropout rate \n",
    "    \"qkv_bias\": False #Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy GPT Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        #placeholder transformer\n",
    "        self.trf_blocks=nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        #placeholder layernorm \n",
    "        self.final_norm =DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_indices = torch.arange(seq_len, device=in_idx.device)\n",
    "        pos_embeds = self.pos_emb(pos_indices)\n",
    "        # pos_embeds=self.pos_emb(in_idx)\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        #placeholder\n",
    "    def forward(self,x):\n",
    "        #nothing happens here just returning the input\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def  __init__(self,normalized_shape,eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "#tokenization \n",
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1=\"Every effort moves you\"\n",
    "txt2=\"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch= torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#create an instance of dummygpt\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits=model(batch)\n",
    "print(\"Output shape:\",logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer normalisation is done so that to prevent the gradient magnitudes to be too large or too small\n",
    "# internal covariate shift : as training proceeds inputs to each layer can change, this delays convergence \n",
    "torch.manual_seed(123)\n",
    "batch_example=torch.randn(2,5)\n",
    "layer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out= layer(batch_example)\n",
    "print(out)\n",
    "# it is done before and after the multi-head attention  module and before final output layer\n",
    "# mean=0 variance =1\n",
    "# this is a linear layer followed by a ReLu activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var=out.var(dim=-1,keepdim=True)\n",
    "print(\"Mean:\",mean)\n",
    "print(\"Variance:\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm =(out - mean)/torch.sqrt(var)\n",
    "mean=out_norm.mean(dim = -1, keepdim = True)\n",
    "var=out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized Layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm (emb_dim=5)\n",
    "out_ln=ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1,keepdim=True)\n",
    "var=out_ln.var(dim=-1,unbiased=False,keepdim=True)\n",
    "print(\"Mean:\\n\",mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gelu activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn =FeedForward(GPT_CONFIG_124M)\n",
    "x=torch.rand(2,3,768)\n",
    "out=ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut=use_shortcut\n",
    "        self.layers=nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU())\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            layer_output=layer(x) #compute output of current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x=x+ layer_output\n",
    "            else:\n",
    "                x= layer_output\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes=[3,3,3,3,3,1]\n",
    "sample_input=torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut= ExampleDeepNeuralNetwork(\n",
    "    layer_sizes,use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(model, x):\n",
    "    output=model(x)#forward pass\n",
    "    target=torch.tensor([[0.]])\n",
    "\n",
    "    loss=nn.MSELoss()#calculate loss\n",
    "    loss=loss(output,target)\n",
    "\n",
    "    loss.backward()#backward pass\n",
    "\n",
    "    for name, param  in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has a gradient of {param.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has a gradient of 0.00020173584925942123\n",
      "layers.1.0.weight has a gradient of 0.00012011159560643137\n",
      "layers.2.0.weight has a gradient of 0.0007152040489017963\n",
      "layers.3.0.weight has a gradient of 0.0013988736318424344\n",
      "layers.4.0.weight has a gradient of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradient(model_without_shortcut, sample_input) # vanish gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has a gradient of 0.22169791162014008\n",
      "layers.1.0.weight has a gradient of 0.20694105327129364\n",
      "layers.2.0.weight has a gradient of 0.32896995544433594\n",
      "layers.3.0.weight has a gradient of 0.2665732204914093\n",
      "layers.4.0.weight has a gradient of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut=ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradient(model_with_shortcut,sample_input) # no vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "multihead attention-->layer normalisation -->Dropout -->Feed Forward layer -->GELU activation \n",
    "##### important components in transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\": 50257, #vocabulary\n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768, #embedding dimension\n",
    "    \"n_heads\": 12, #no of attention heads\n",
    "    \"n_layers\": 12, #no of layers\n",
    "    \"drop_rate\": 0.1, #dropout rate \n",
    "    \"qkv_bias\": False #Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the separate elements youve added\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "x=torch.rand(2,4,768)\n",
    "block=TransformerBlock(GPT_CONFIG_124M)\n",
    "output=block(x)\n",
    "print(\"Input Shape:\",x.shape)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks=nn.Sequential(\n",
    "           *[TransformerBlock(cfg) for  _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm=LayerNorm (cfg[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(\n",
    "            cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape\n",
    "        tok_embeds=self.tok_emb(in_idx)\n",
    "        pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
    "        x=tok_embeds+pos_embeds\n",
    "        x=self.drop_emb(x)\n",
    "        x=self.trf_blocks(x)\n",
    "        x=self.final_norm(x)\n",
    "        logits=self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "batch=[]\n",
    "txt1=\"Every effort moves you\"\n",
    "txt2=\"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch= torch.stack(batch,dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      " Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "out=model(batch)\n",
    "print(\"Input batch:\\n\",batch)\n",
    "print(\"\\n Output shape:\",out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 163009536\n"
     ]
    }
   ],
   "source": [
    "total_params=sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\",total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the next token\n",
    "### extract last vector and convert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "    # idx is (batch,n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:] #if input size > context size, we select only the lasr elements the size of context size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits =model(idx_cond) #get predictions \n",
    "        \n",
    "        logits=logits[:,-1,:] # last row \n",
    "\n",
    "        # convert logits into probabilities by applying softmax\n",
    "\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        # Get the idx of the vocab entry with highest priority\n",
    "        idx_next =torch.argmax(probas, dim =-1 , keepdim =True )\n",
    "\n",
    "        # append it to the running sequence \n",
    "\n",
    "        idx= torch.cat((idx,idx_next),dim=-1)\n",
    "\n",
    "    return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor_shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context =\"Hello, I am\"\n",
    "encoded=tokenizer.encode(start_context)\n",
    "print(\"encoded:\",encoded)\n",
    "encoded_tensor=torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor_shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output Len 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #set  model to evaluation mode\n",
    "out=generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\",out)\n",
    "print(\"Output Len\",len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text=tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text) # the output is random and messy because the model is not trained yet \n",
    "# next we will train this model for more efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining\n",
    "### Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "GPT_CONFIG_124M={\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_heads\":12,\n",
    "    \"n_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model= GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # disable dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded =tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # adding batch dimension \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0) #remove batch dimension \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context =\"Every effort moves you\"\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids =generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=torch.tensor([[16833,3626,6100], #effort really moves\n",
    "                     [40,1107,588]]) # I really like\n",
    "\n",
    "targets=torch.tensor([[3626,6100,345], # effort moves you \n",
    "                       [1107,588,11311]])  #really likes choclate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits=model(inputs)\n",
    "probas=torch.softmax(logits,dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids=torch.argmax(probas,dim=-1,keepdim=True)\n",
    "print(\"Token IDs:\\n\",token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target batch 1:  effort moves you\n",
      "Output batch 1:  pressuring empoweredfaith\n"
     ]
    }
   ],
   "source": [
    "print(\"Target batch 1:\",token_ids_to_text(targets[0],tokenizer))\n",
    "print(\"Output batch 1:\",token_ids_to_text(token_ids[1].flatten(),tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip\n",
    "to reduce loss what we do is that we would flatten the output tensor and then flatten the target tensor and then \n",
    "check the probability of each element in the target tensor in the output tensor in the prefect case the probability should be one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx=0\n",
    "target_probas_1 =probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 1:\",target_probas_1) #p11 , p12 p13\n",
    "\n",
    "text_idx=1\n",
    "target_probas_2 =probas[text_idx,[0,1,2],targets[text_idx]]\n",
    "print(\"Text 2:\",target_probas_2) # p21, p22, p23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# concatenate them \n",
    "log_probas=torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# calculate mean of the log values\n",
    "avg_log_probas=torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# taking negative of the avg log\n",
    "neg_avg_log_probas=avg_log_probas * -1\n",
    "print(neg_avg_log_probas)# its conventional in deep learning to use negative loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Logits: torch.Size([6, 50257])\n",
      "Flattened Targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#much simpler way of doing this\n",
    "logits_flat=logits.flatten(0,1)\n",
    "targets_flat=targets.flatten()\n",
    "print(\"Flattened Logits:\",logits_flat.shape)\n",
    "print(\"Flattened Targets:\",targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# with just one line of code we get the loss\n",
    "loss= torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "# perplexity measures how well the probability distribution predicted by the model matches the actual distribution of words in the dataset\n",
    "# perplexity =exp(loss)\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "#using the verdict to train this\n",
    "import os\n",
    "file_path=\"the-verdict.txt\"\n",
    "with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
    "    text_data=file.read()\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "\n",
    "tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
    "total_char=len(text_data)\n",
    "total_tokens=len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\",total_char)\n",
    "print(\"Tokens:\",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt,batch_size=4,  #batch_size=4 no of elements in a training batch , stride=128 means the amount of movement of the window\n",
    "            max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0): # drop last is done so that the last batch of the training set may be ignored as if the batch size< 4 causes problem num_workers is the number of cpu threads that can be run simultaneously\n",
    "            tokenizer=tiktoken.get_encoding(\"gpt2\") #tokenizer initiated\n",
    "            dataset=GPTDatasetV1(txt,tokenizer,max_length,stride) #dataset made\n",
    "            dataloader= DataLoader( #create dataloader\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                drop_last=drop_last,   #this dataloaderV1 and then from getitems it gives out a input output\n",
    "                num_workers=num_workers\n",
    "            )\n",
    "            return dataloader\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # we have to group by the number of heads\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "#combine the separate elements youve added\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-5\n",
    "        self.scale=nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        # when unbiased= True then we get bessels correction where the denominator is n-1 not n \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "       super().__init__()\n",
    "    def forward (self,x):\n",
    "       return 0.5 * x * (1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+ 0.044715 * torch.pow(x,3))))  \n",
    "        \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"],4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att=MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff=FeedForward(cfg)\n",
    "        self.norm1=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut=nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        shortcut=x\n",
    "        x=self.norm1(x)\n",
    "        x=self.att(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        shortcut=x\n",
    "        x=self.norm2(x)\n",
    "        x=self.ff(x)\n",
    "        x=self.drop_shortcut(x)\n",
    "        x=x+shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)\n",
    "        val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens,context_size):\n",
    "    # idx is (batch,n_tokens)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:] #if input size > context size, we select only the lasr elements the size of context size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits =model(idx_cond) #get predictions \n",
    "        \n",
    "        logits=logits[:,-1,:] # last row \n",
    "\n",
    "        # convert logits into probabilities by applying softmax\n",
    "\n",
    "        probas = torch.softmax(logits,dim=-1)\n",
    "\n",
    "        # Get the idx of the vocab entry with highest priority\n",
    "        idx_next =torch.argmax(probas, dim =-1 , keepdim =True )\n",
    "\n",
    "        # append it to the running sequence \n",
    "\n",
    "        idx= torch.cat((idx,idx_next),dim=-1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "def text_to_token_ids(text,tokenizer):\n",
    "    encoded =tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor=torch.tensor(encoded).unsqueeze(0) # adding batch dimension \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids,tokenizer):\n",
    "    flat=token_ids.squeeze(0) #remove batch dimension \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context =\"Every effort moves you\"\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids =generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context,tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
    "    model.eval()\n",
    "    context_size=model.pos_emb.weight.shape[0]\n",
    "    encoded=text_to_token_ids(start_context,tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids=generate_text_simple(\n",
    "            model=model,idx=encoded,\n",
    "            max_new_tokens=50,context_size=context_size\n",
    "        )\n",
    "    decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\",\" \"))\n",
    "    model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,eval_freq,eval_iter,start_context,tokenizer):\n",
    "    train_losses,val_losses, track_tokens_seen =[],[],[]\n",
    "    tokens_seen,global_step=0,-1\n",
    "\n",
    "    #Main training loop \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()# set model to training mode\n",
    "        for input_batch,target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
    "            loss.backward()#calculate loss gradients\n",
    "            optimizer.step()# update model weights using loss gradients\n",
    "            tokens_seen+=input_batch.numel() #returns total number of tokens\n",
    "            global_step+=1\n",
    "\n",
    "            #evaluation\n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss,val_loss=evaluate_model(\n",
    "                    model,train_loader,val_loader,device,eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}):\"\n",
    "                      f\"Train Loss {train_loss:.3f}, Val Loss {val_loss:.3f}\")\n",
    "        # print sample after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model,tokenizer,device,start_context\n",
    "        )\n",
    "    return train_losses,val_losses,track_tokens_seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000):Train Loss 9.781, Val Loss 9.933\n",
      "Ep 1 (Step 000005):Train Loss 8.111, Val Loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010):Train Loss 6.661, Val Loss 7.048\n",
      "Ep 2 (Step 000015):Train Loss 5.961, Val Loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020):Train Loss 5.726, Val Loss 6.600\n",
      "Ep 3 (Step 000025):Train Loss 5.201, Val Loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030):Train Loss 4.417, Val Loss 6.278\n",
      "Ep 4 (Step 000035):Train Loss 4.069, Val Loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040):Train Loss 3.732, Val Loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045):Train Loss 2.850, Val Loss 6.179\n",
      "Ep 6 (Step 000050):Train Loss 2.427, Val Loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055):Train Loss 2.104, Val Loss 6.134\n",
      "Ep 7 (Step 000060):Train Loss 1.882, Val Loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065):Train Loss 1.320, Val Loss 6.238\n",
      "Ep 8 (Step 000070):Train Loss 0.985, Val Loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075):Train Loss 0.717, Val Loss 6.293\n",
      "Ep 9 (Step 000080):Train Loss 0.541, Val Loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085):Train Loss 0.391, Val Loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training Completed in 19.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\n",
    "\n",
    "num_epochs=10\n",
    "train_losses,val_losses,tokens_seen= train_model_simple(\n",
    "    model,train_loader,val_loader,optimizer,device,num_epochs=num_epochs,\n",
    "    eval_freq=5,eval_iter=5,start_context=\"Every effort moves you\",tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time=time.time()\n",
    "execution_time_minutes=(end_time - start_time)/60\n",
    "print(f\"Training Completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats your name? It was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(\n",
    "            model,tokenizer,device,\"whats your name?\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Strats to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids=generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategy 1: Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use multinomial sampling\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "next_token_logits2 = next_token_logits/0.1\n",
    "\n",
    "next_token_logits3 = next_token_logits/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8530e-10, 3.5189e-26, 2.6890e-38, 9.9099e-01, 5.7569e-23, 4.4220e-37,\n",
      "        2.9718e-38, 9.0133e-03, 2.8514e-22])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits2, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits3, dim=0)\n",
    "\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.0907e-02, 1.6313e-03, 1.0019e-04, 5.7212e-01, 3.4190e-03, 1.3257e-04,\n",
      "        1.0120e-04, 3.5758e-01, 4.0122e-03])\n",
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(next_token_id)\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "##Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Strategy 2: Top-k Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k=3\n",
    "top_logits,top_pos=torch.topk(next_token_logits,top_k)\n",
    "print(\"Top logits:\",top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits=torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas=torch.softmax(new_logits,dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging topk and temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,idx,max_new_tokens,context_size,temperature=0.0,top_k=None,eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond=idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits=model(idx_cond)\n",
    "        logits=logits[:,-1,:]\n",
    "\n",
    "        if top_k is not None:\n",
    "            # keep topk values\n",
    "            top_logits,_ = torch.topk(logits,top_k)\n",
    "            min_val=top_logits[:,-1]\n",
    "            logits=torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
    "\n",
    "\n",
    "        #apply temp scaling\n",
    "        if temperature >0.0:\n",
    "            logits=logits/temperature\n",
    "\n",
    "            # softmax\n",
    "            probs=torch.softmax(logits,dim=-1) # (batch_size,context_len)\n",
    "\n",
    "            #sample from distribution \n",
    "            idx_next=torch.multinomial(probs,num_samples=1)\n",
    "        \n",
    "        #otherwise same as before\n",
    "        else:\n",
    "            idx_next=torch.argmax(logits,dim=-1,keepdim=True)\n",
    "        \n",
    "\n",
    "        if idx_next== eos_id:\n",
    "            break\n",
    "        \n",
    "        # append sampled text to the sequence\n",
    "        idx=torch.cat((idx,idx_next),dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand to work on surprise, a one of us had gone with random-\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids=generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\",tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\",token_ids_to_text(token_ids,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
